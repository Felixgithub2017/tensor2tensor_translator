{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Translation\n",
    "Authors\n",
    "- Alexander Wolf\n",
    "- 11/4/17\n",
    "\n",
    "This is a notebook showcasing how you can make a translator via the [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) library. The Tensor2Tensor library was made to be buit with binaries and used in the temrinal; however, this notebok reveresed engineered the library a bit to allow you to train T2T models in code. The API supplies datasets and code to make translators in the following languages.\n",
    "\n",
    "1. French\n",
    "2. German\n",
    "3. Chinese\n",
    "4. Chzech\n",
    "5. Macedonian\n",
    "\n",
    "Tensor2Tensor consists of three main components to train advanced neural networks with minimal code.\n",
    "\n",
    "1. Problems\n",
    "\t- This specifies what type of input modality the network will have and what your task your trying to solve. \n",
    "2. Model\n",
    "\t- T2T consist of many state of the art models for translation and other tasks, which are independent of the type of input/output modality. For translation tasks it is recommended to use the *Transformer*. \n",
    "3. Hyperparameter Sets\n",
    "    - These are predefined hparam sets to train model on the specified T2T problem.They can be modified and changed also.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import trainer_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models, problems\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Notebook Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define T2T Problem, Model, and HPARAMS\n",
    "These are Tensor2Tensor parameters which will be used to make the correct pipeline for training, evaluating, and decoding your T2T model properly. Red more below to see how to set them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "A Tensor2Tensor translation problems specifies what language you're translating from and to, along with the vocabulary source and size for your translation model. This will be needed to be set for data collection, preparation, and training your model. Add '_rev' to the end of the problem name to translate from a language to english instead of english to the other selected langauge. \n",
    "\n",
    "In Tensor2Tensor the format of a translation problem name goes as follows\n",
    "- 'translate_en' + OTHER_LANGAUGE_CODE + 'wmt' + VOCAB_SIZE/TYPE *+ (optional) + '_rev'*\n",
    "\n",
    "You can view all the available problems in T2T by executing the code below. It is possible to add your own problem to T2T also if it is not already built in. See [here](https://github.com/tensorflow/tensor2tensor/blob/master/docs/new_problem.md) for more details\n",
    "\n",
    "```python\n",
    "from tensor2tensor import problems\n",
    "problems.available()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PARAMS['T2T_Problem'] = 'translate_enfr_wmt32k_rev' # T2T Problem name for French -> English model with 32k word vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Tensor2Tensor has a lot of prebuilt code for neural sequential data processing. For translation, it is advised to use the *'transformer'* model; it is able to train very quickly compared to other models like RNNS, and converges well.\n",
    "\n",
    "To see all available T2T models execute the code below; Note not all of these can be used for translation tasks\n",
    "```python\n",
    "from tensor2tensor import registry\n",
    "registry.list_models()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PARAMS['T2T_Model'] = 'transformer' # Setting the Transformer Model for Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HPARAM Set\n",
    "\n",
    "Tensor2Tensor contains different prebuilt hyperparameter sets for defining the number of hidden layers in the model, learning rates, regularization terms, and much more. They are already preconfigured to help you produce state of the art models, but can be modified to how ever you want within the notebook later.\n",
    "\n",
    "For translation if using 1 GPU, it is advised to start off with hparam set *transformer_base*. To view all prebuilt hparams sets for your model you can look at the Tensor2Tensor source code: For example at the bottom of this [py file](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) you can find all the Transformer hparam sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PARAMS['T2T_HPARAMS'] = 'transformer_base' # Setting the Transformer Model for Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Problem\n",
    "Tensor2Tensor supplies data genrators for each problem in its library. You will need to generate the data into a *Data_Dir* only once. This process can take many hours so by default the function which generates the data is commented out.\n",
    "\n",
    "Make sure to import the correct data generator for the problem your using in the T2T library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.data_generators.translate_enfr import TranslateEnfrWmt32k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Temp and Data  Directory\n",
    "Set these two directories below.\n",
    "- *TEMP_DIR*\n",
    "\t- Stores downloaded zip files of text data from internet\n",
    "\t- Can be reused for any type of translation problem\n",
    "- *DATA_DIR*\n",
    "\t- This directory precompiles the data for your specific T2T problem\n",
    "\t- You will need to set a new one for each new translation task\n",
    "    - Can be reused in training again if the same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS['TMP_DIR'] = '/data.nfs/awolf/translation/wmt_t2t_data/tmp/t2t_datagen' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS['DATA_DIR'] = '/data.nfs/awolf/translation/enfr_rev_train_data/translate_fr_en_wmt32k' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now Generate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TranslateEnfrWmt32k.generate_data(DATA_DIR, TMP_DIR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Train Directory\n",
    "You need to set a Train Dir which Tensor2Tensor will use to store the model checkpoints in. This can be used to load models and continue training from those checkpoints if stopped. The checkpoint files can get very large, because there are so many parameters in the model, so keep that in mind when selecting a directory.\n",
    "\n",
    "Tensor2Tensor will *overwrite* model checkpoints stored in this directory, so if you don't want your old checkpoints erased, make sure to change this directory when training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS['TRAIN_DIR'] = '/data.nfs/awolf/translation/models/translate_enfr_wmt32k_rev/transformer-transformer_base' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get TF Hparams Object\n",
    "Tensor2Tensor creates a [Tensorflow Hparams](https://www.tensorflow.org/api_docs/python/tf/contrib/training/HParams) object for training. Make the onject using Tensor2Tensor. Make any edits or changes below also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hparams = trainer_lib.create_hparams(PARAMS['T2T_HPARAMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View your hparams, make any changes as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams.batch_size = 1024\n",
    "hparams.learning_rate_warmup_steps = 45000\n",
    "hparams.learning_rate = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'attention_dropout': 0.1,\n",
       " u'attention_dropout_broadcast_dims': u'',\n",
       " u'attention_key_channels': 0,\n",
       " u'attention_value_channels': 0,\n",
       " u'batch_size': 1024,\n",
       " u'clip_grad_norm': 0.0,\n",
       " u'compress_steps': 0,\n",
       " u'daisy_chain_variables': True,\n",
       " u'dropout': 0.2,\n",
       " u'eval_drop_long_sequences': False,\n",
       " u'eval_run_autoregressive': False,\n",
       " u'factored_logits': False,\n",
       " u'ffn_layer': u'dense_relu_dense',\n",
       " u'filter_size': 2048,\n",
       " u'force_full_predict': False,\n",
       " u'grad_noise_scale': 0.0,\n",
       " u'hidden_size': 512,\n",
       " u'initializer': u'uniform_unit_scaling',\n",
       " u'initializer_gain': 1.0,\n",
       " u'input_modalities': u'default',\n",
       " u'kernel_height': 3,\n",
       " u'kernel_width': 1,\n",
       " u'label_smoothing': 0.1,\n",
       " u'layer_postprocess_sequence': u'da',\n",
       " u'layer_prepostprocess_dropout': 0.1,\n",
       " u'layer_prepostprocess_dropout_broadcast_dims': u'',\n",
       " u'layer_preprocess_sequence': u'n',\n",
       " u'learning_rate': 0.4,\n",
       " u'learning_rate_cosine_cycle_steps': 250000,\n",
       " u'learning_rate_decay_rate': 1.0,\n",
       " u'learning_rate_decay_scheme': u'none',\n",
       " u'learning_rate_decay_staircase': False,\n",
       " u'learning_rate_decay_steps': 5000,\n",
       " u'learning_rate_minimum': None,\n",
       " u'learning_rate_schedule': u'linear_warmup_rsqrt_decay',\n",
       " u'learning_rate_warmup_steps': 45000,\n",
       " u'length_bucket_step': 1.1,\n",
       " u'max_input_seq_length': 0,\n",
       " u'max_length': 256,\n",
       " u'max_relative_position': 0,\n",
       " u'max_target_seq_length': 0,\n",
       " u'min_length': 0,\n",
       " u'min_length_bucket': 8,\n",
       " u'moe_hidden_sizes': u'2048',\n",
       " u'moe_k': 2,\n",
       " u'moe_loss_coef': 0.01,\n",
       " u'moe_num_experts': 64,\n",
       " u'multiply_embedding_mode': u'sqrt_depth',\n",
       " u'nbr_decoder_problems': 1,\n",
       " u'no_data_parallelism': False,\n",
       " u'norm_epsilon': 1e-06,\n",
       " u'norm_type': u'layer',\n",
       " u'num_decoder_layers': 0,\n",
       " u'num_encoder_layers': 0,\n",
       " u'num_heads': 8,\n",
       " u'num_hidden_layers': 6,\n",
       " u'optimizer': u'Adam',\n",
       " u'optimizer_adam_beta1': 0.9,\n",
       " u'optimizer_adam_beta2': 0.997,\n",
       " u'optimizer_adam_epsilon': 1e-09,\n",
       " u'optimizer_momentum_momentum': 0.9,\n",
       " u'optimizer_momentum_nesterov': False,\n",
       " u'parameter_attention_key_channels': 0,\n",
       " u'parameter_attention_value_channels': 0,\n",
       " u'pos': u'timing',\n",
       " u'prepend_mode': u'none',\n",
       " u'problem_choice': u'adaptive',\n",
       " u'proximity_bias': False,\n",
       " u'relu_dropout': 0.1,\n",
       " u'relu_dropout_broadcast_dims': u'',\n",
       " u'sampling_method': u'argmax',\n",
       " u'sampling_temp': 1.0,\n",
       " u'scheduled_sampling_gold_mixin_prob': 0.5,\n",
       " u'scheduled_sampling_prob': 0.0,\n",
       " u'scheduled_sampling_warmup_steps': 50000,\n",
       " u'self_attention_type': u'dot_product',\n",
       " u'shared_embedding_and_softmax_weights': True,\n",
       " u'split_to_length': 0,\n",
       " u'summarize_grads': False,\n",
       " u'summarize_vars': False,\n",
       " u'symbol_dropout': 0.0,\n",
       " u'symbol_modality_num_shards': 16,\n",
       " u'symbol_modality_skip_top': False,\n",
       " u'target_modality': u'default',\n",
       " u'use_fixed_batch_size': False,\n",
       " u'use_pad_remover': True,\n",
       " u'weight_decay': 0.0,\n",
       " u'weight_noise': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(hparams.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Options for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep these flags as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS.problems = PARAMS['T2T_Problem']\n",
    "FLAGS.model = PARAMS['T2T_Model']\n",
    "FLAGS.schedule = \"train_and_evaluate\"\n",
    "FLAGS.local_eval_frequency = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Eval Steps\n",
    "How many local steps to train the model for and how many to evaulate for. It is easier to set a high amont train_steps and then stop training when you reach a desired performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS['train_steps'] = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/ Distributed Training Flags\n",
    "- Use these to control sing;e and multi GPU support\n",
    "- For more info seee Tensor2Tensor's [documentation](https://github.com/tensorflow/tensor2tensor/blob/master/docs/distributed_training.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Set this to contrl which GPU is visable during training\n",
    "PARAMS['keep_checkpoint_max'] = 3      # Number of checkpoints to keep in TRAIN_DIR before overwriting the oldest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS.gpu_memory_fraction = .99\n",
    "FLAGS.keep_checkpoint_every_n_hours = 10000  \n",
    "FLAGS.save_checkpoints_secs = 0\n",
    "FLAGS.worker_gpu = 1\n",
    "FLAGS.ps_gpu = 2\n",
    "FLAGS.log_device_placement = True\n",
    "FLAGS.worker_replicas = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tensorflow Experiment Object\n",
    "Tensor2Tensor does the training with Tensorflow by creating an [experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment) for you. \n",
    "\n",
    "First thing we need to do is create the run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n"
     ]
    }
   ],
   "source": [
    "RUN_CONFIG = create_run_config(\n",
    "      model_dir=PARAMS['TRAIN_DIR'],\n",
    "      keep_checkpoint_max=PARAMS['keep_checkpoint_max'],\n",
    "      keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n",
    "      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n",
    "      gpu_mem_fraction=FLAGS.gpu_memory_fraction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing the Model with Tensor2Tensor\n",
    "Check logs below to see progress of model loss and BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATA_DIR': '/data.nfs/awolf/translation/enfr_rev_train_data/translate_fr_en_wmt32k',\n",
       " 'T2T_HPARAMS': 'transformer_base',\n",
       " 'T2T_Model': 'transformer',\n",
       " 'T2T_Problem': 'translate_enfr_wmt32k_rev',\n",
       " 'TMP_DIR': '/data.nfs/awolf/translation/wmt_t2t_data/tmp/t2t_datagen',\n",
       " 'TRAIN_DIR': '/data.nfs/awolf/translation/models/translate_enfr_wmt32k_rev/transformer-transformer_base',\n",
       " 'keep_checkpoint_max': 3,\n",
       " 'train_steps': 1000000}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training function twice without resetting the notebook kernel, you'll need to run the commented out code below before training for the second or more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del hparams.data_dir\n",
    "# del hparams.train_steps\n",
    "# del hparams.eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 3, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x84b3450>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.99\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': '/data.nfs/awolf/translation/models/translate_enfr_wmt32k_rev/transformer-transformer_base', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x35df690>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0xb7b41b8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using ValidationMonitor\n",
      "WARNING:tensorflow:From /data/dataiku/dss-data-dir/code-envs/python/Tensor2Tensor_GPU/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Reading data files from /data.nfs/awolf/translation/enfr_rev_train_data/translate_fr_en_wmt32k/translate_enfr_wmt32k-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dataiku/dss-data-dir/code-envs/python/Tensor2Tensor_GPU/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:509: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_27664_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_27664_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_27664_512.top\n",
      "INFO:tensorflow:Base learning rate: 0.400000\n",
      "INFO:tensorflow:Trainable Variables Total size: 58284032\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /data.nfs/awolf/translation/models/translate_enfr_wmt32k_rev/transformer-transformer_base/model.ckpt-394009\n",
      "INFO:tensorflow:Saving checkpoints for 394010 into /data.nfs/awolf/translation/models/translate_enfr_wmt32k_rev/transformer-transformer_base/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "exp_fn = trainer_lib.create_experiment(\n",
    "        run_config=RUN_CONFIG,\n",
    "        hparams=hparams,\n",
    "        model_name=PARAMS['T2T_Model'],\n",
    "        problem_name=PARAMS['T2T_Problem'],\n",
    "        data_dir=PARAMS['DATA_DIR'],\n",
    "        train_steps=PARAMS['train_steps'],\n",
    "        eval_steps=50\n",
    "    )\n",
    "exp_fn.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creator": "admin",
  "kernelspec": {
   "display_name": "Python (env Tensor2Tensor_GPU)",
   "language": "python",
   "name": "py-dku-venv-tensor2tensor_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
